---
title: "R Notebook of E14 downstream analysis from binarized peak level data with "
output: html_notebook
---
# R Packages

```{r Load packagies, include=FALSE}
if (Sys.info()['sysname']=="Linux") {
    .libPaths(c("/projappl/project_2001539/project_rpackages", .libPaths()))
    libpath <- .libPaths()[1]
}

load.libs <- c(
  "viridisLite",
  "DT",
  "GenomicRanges",
  "data.table",
  "plyr",
  "gplots",
  "magrittr",
  "Matrix",
  "proxy",
  "qs",
  "genomation",
  "densityClust",
  "irlba",
  "umap",
  "clusterProfiler",
  "RColorBrewer",
  "openxlsx",
  "gridExtra",
  "Signac",
  "Seurat",
  "regioneR",
  "reldist",
  "SeuratWrappers",
  "chromVAR",
  "tidyverse",
  "TFBSTools",
  "BSgenome.Mmusculus.UCSC.mm10",
  "EnsDb.Mmusculus.v79",
  "org.Mm.eg.db",
  "plotly",
  "ggplotify",
  "motifmatchr",
  "future",
  "cowplot",
  "universalmotif",
  "readxl")
if (!require("pacman")) install.packages("pacman"); library(pacman)
p_load(load.libs, update = FALSE, character.only = TRUE, install = FALSE)
status <- sapply(load.libs,require,character.only = TRUE)
if(all(status)){
    print("SUCCESS: You have successfully installed and loaded all required libraries.")
} else{
    cat("ERROR: One or more libraries failed to install correctly. Check the following list for FALSE cases and try again...\n\n")
    status
}

source("../../generic code/AuxFunctions.R")
set.seed(2020);
```

# Reading datafiles

```{r Setwd sample name specific variables, message=FALSE}
sample.name <- "E14"
run.date <- "2808222"
supp_data_path <- "~/Workspace/mm10/"
cores=6
std.aim <- 0.15
```

```{r Setting Seurat multicore}
plan("multisession", workers = cores)
options(future.globals.maxSize = 12 * 1024 ^ 3, future.seed=TRUE, future.rng.onMisuse="ignore")
```

```{r Reading in data objects}
s.data <- readRDS(paste("../scATAC_data/",sample.name,".merged.peaks.298622.Rds",sep=""))
s.data_RNA <- readRDS("../scRNA_data/E14scRNAseq.300822.Rds")
```

```{r TFIDF and SVD, message=FALSE}
DefaultAssay(s.data) <- 'peaks'

set.seed(2020)

s.data <- RunTFIDF(s.data, method=3)
s.data <- FindTopFeatures(s.data, min.cutoff = 'q5')
s.data <- RunSVD(
  object = s.data,
  assay = 'peaks',
  reduction.key = 'LSI_',
  reduction.name = 'lsi',
  n=60
)
```

```{r Plot depth dimension correlation}
DepthCor(s.data)
```

```{r Calculate proportion of stdebv covered by LSI components in comparison to 2}
std.proportion <- sapply(2:length(s.data@reductions$lsi@stdev),function(x){s.data@reductions$lsi@stdev[x]/s.data@reductions$lsi@stdev[2]})

# Finding number of components of which std proportion to 2nd component is closest to std.aim
max.lsi.dim <- which.min(abs(std.proportion-std.aim))
```

# scATAC UMAP clusters

```{r UMAP and community detection, fig.width=14, fig.height=14}
# Initial UMAP

s.data <- RunUMAP(object = s.data, reduction = 'lsi', dims = 2:max.lsi.dim)
s.data <- FindNeighbors(object = s.data, reduction = 'lsi', dims = 2:max.lsi.dim)
suppressWarnings(s.data <- FindClusters(object = s.data, verbose = FALSE, algorithm = 4, resolution = 4))
DimPlot(object = s.data, label = TRUE, pt.size=1.2, label.size = 8) + NoLegend()
```

```{r Plot replicate distribution, fig.width=14, fig.height=14}
DimPlot(object = s.data, label = TRUE, pt.size=.8,group.by="replicate", shuffle =TRUE, label.size = 8) + NoLegend()
```

# RNA integration

```{r Create estimated gene activity assay}
# Let's extract genomic annotation metadata from s.data[['peaks']]
genomic.metadata <- mcols(Annotation(s.data[['peaks']]))
# Generate conversion table from gene_name to gene_id
gene_name2gene_id <- as_tibble(genomic.metadata[,c("gene_name","gene_id")])

# Calculate gene activity estimate from scATAC reads based on the scATAC, by using gene_names as function does not support any other id
gene.activities <- GeneActivity(s.data, assay="peaks")

# Store gene_names
gene.names <- rownames(gene.activities)

# Switch sparse matrix to use ensmusg id
ensmusg.ids <- gene_name2gene_id[match(gene.names,pull(gene_name2gene_id,"gene_name")),] %>% pull("gene_id")
gene_names <- gene_name2gene_id[match(gene.names,pull(gene_name2gene_id,"gene_name")),] %>% pull("gene_name")

# Dropping NAs
non.na.i <- !is.na(ensmusg.ids)
gene.activities.gene_id <- gene.activities[non.na.i,]
rownames(gene.activities.gene_id) <- ensmusg.ids[non.na.i]

# Add the gene activity matrix to the Seurat object as a new assay
s.data[['Activity']] <- CreateAssayObject(counts = gene.activities.gene_id)
s.data <- NormalizeData(
  object = s.data,
  assay = 'Activity',
  normalization.method = 'LogNormalize',
  scale.factor = median(s.data$nCount_Activity)
)

# Add gene_name to gene_id mapping into the s.data[['Activity']] assays metadata
s.data[['Activity']]<- AddMetaData(s.data[['Activity']], col.name = "feature_symbol", metadata = gene_names[non.na.i])
```

```{r Read marker gene list and do FeaturePlot based on gene activity, fig.height=20, fig.width=20, message=FALSE}
neuronal.markers<- read_tsv("../../CellAnnotation/CellTypeMarkers_E14.tsv", col_names = c("annotation","geneName"))
feature.metadata <- s.data[['Activity']][[]] %>% rownames_to_column(var="gene_id") %>% as_tibble()
neuronal.markers.tmp <- filter(feature.metadata, feature_symbol %in% neuronal.markers$geneName)

neuronal.markers.ids <- pull(neuronal.markers.tmp,"gene_id")
neuronal.markers.names <- pull(neuronal.markers.tmp,"feature_symbol")

DefaultAssay(s.data) <- 'Activity'

f.plot.tmp <- FeaturePlot(
  object = s.data,
  features = neuronal.markers.ids,
  pt.size = 0.1,
  max.cutoff = 'q95',
  combine = F
)

f.plots.1 <- lapply(1:length(f.plot.tmp),function(i){
  f.plot.tmp[[i]] + labs(title=neuronal.markers.names[i])
})

patchwork::wrap_plots(f.plots.1)
```

```{r Perform label transfer}
s.data_RNA@meta.data$CellType<-s.data_RNA@meta.data$seurat_cluster

# Finding transfer anchors
transfer.anchors <- FindTransferAnchors(
    reference = s.data_RNA,
    query = s.data,
    reduction = 'cca',
    reference.assay="RNA",
    query.assay = "Activity",
    features = VariableFeatures(object=s.data_RNA)
)

predicted.labels <- TransferData(
  anchorset = transfer.anchors,
  refdata = s.data_RNA@meta.data$CellType,
  weight.reduction = s.data[['lsi']],
  dims = 2:max.lsi.dim
)

s.data <- AddMetaData(object = s.data, metadata = predicted.labels)
```

```{r Plot prediction scores, echo=FALSE}
pred.score.df <- data.frame(pred.score=s.data$prediction.score.max)
ggplot(pred.score.df, aes(x=pred.score)) + geom_histogram(binwidth=.025) + geom_vline(data=pred.score.df, aes(xintercept=0.5, color="red"),linetype="dashed") + theme(legend.position = 'none')
```

```{r Calculate accepted prediction score fraction}
prediction.score.over.th <- table(s.data$prediction.score.max > 0.5)
p.freq <- prediction.score.over.th['TRUE']/(prediction.score.over.th['TRUE']+prediction.score.over.th['FALSE'])

p.score.th <- as.numeric(prediction.score.over.th)
val_names <- sprintf("%s (%s)", c("Match not found", "Match found"), scales::percent(round(p.score.th/sum(p.score.th), 2)))
val_names
```

```{r Perform scRNA data imputation, message=FALSE, include = FALSE}
refdata <- GetAssayData(s.data_RNA, assay = "RNA", slot = "data")

s.data_RNA@meta.data$tech<-"scRNA"
s.data@meta.data$tech<-"scATAC"

imputation <- TransferData(anchorset = transfer.anchors, refdata = refdata, weight.reduction = s.data[["lsi"]], dims = 2:max.lsi.dim)

s.data[["RNA"]] <- imputation
coembed <- merge(x = s.data_RNA, y = s.data)

# Copy feature metadata from s.data_rna to s.data
s.data_rna.feature.metadata <- s.data_RNA[["RNA"]][[]]
s.data[["RNA"]] <- AddMetaData(s.data[["RNA"]], metadata = s.data_rna.feature.metadata[rownames(s.data[["RNA"]]),"feature_symbol"], col.name = "feature_symbol")

# Find variable features
coembed <- FindVariableFeatures(coembed)

# Finally, we run PCA and UMAP on this combined object, to visualize the co-embedding of both datasets
coembed <- ScaleData(coembed, do.scale = FALSE)
coembed <- RunPCA(coembed, verbose = FALSE)
coembed <- RunUMAP(coembed, dims = 2:30)
coembed@meta.data$CellType <- ifelse(!is.na(coembed@meta.data$CellType), coembed@meta.data$CellType, coembed@meta.data$predicted.id)
```

    Finding integration vectors

    Finding integration vector weights

    Transfering 27999 features onto reference data

    Centering data matrix

    11:43:23 UMAP embedding parameters a = 0.9922 b = 1.112

    11:43:23 Read 10354 rows and found 29 numeric columns

    11:43:23 Using Annoy for neighbor search, n_neighbors = 30

    11:43:23 Building Annoy index with metric = cosine, n_trees = 50

    0%   10   20   30   40   50   60   70   80   90   100%

    [----|----|----|----|----|----|----|----|----|----|

    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *
    *

    |

    11:43:28 Writing NN index file to temp file /var/folders/yb/wvk8t07s719136klxnc8nmp9sglf93/T//Rtmpnq8Zdj/file72653e114263

    11:43:28 Searching Annoy index using 1 thread, search_k = 3000

    11:43:32 Annoy recall = 100%

    11:43:32 Commencing smooth kNN distance calibration using 1 thread

    11:43:35 Initializing from normalized Laplacian + noise

    11:43:35 Commencing optimization for 200 epochs, with 432826 positive edges

    11:43:45 Optimization finished

```{r Plot coembed data based on technology and cell type, cache=TRUE}
p1 <- DimPlot(coembed, group.by = "tech", shuffle=TRUE)
p2 <- DimPlot(coembed, group.by = "CellType", label = TRUE, repel = TRUE)  + theme(legend.position="none")

p1 + p2
```

```{r Filtering Seurat data object based on prediction score for all downstream analysis}
s.data <- subset(s.data, subset = prediction.score.max > 0.5)
```

```{r Storing old ident set}
DefaultAssay(s.data) <- "peaks"
s.data[['All_ATAC_idents']] <- Idents(s.data)
```

# Calculate NT typing for clusters

```{r NT typing preparation}
source("https://raw.githubusercontent.com/IanevskiAleksandr/sc-type/master/R/gene_sets_prepare.R")
source("https://raw.githubusercontent.com/IanevskiAleksandr/sc-type/master/R/sctype_score_.R")

NT.type.markers <- gene_set_prepare2("/Users/kilpinen/OneDrive - University of Helsinki/Neuronal Feature Space/onlyNT_markers280722.xlsx", "Brain")

#Renaming genes as gene names instead of ENSMUSG IDs
s.data<-RenameGenesSeurat(s.data,s.data[["RNA"]]@meta.features[,1])
DefaultAssay(s.data) <- 'RNA_name'

#Creating scores for artificial genes: s & g2m
s.data <- CellCycleScoring(
  object = s.data,
  g2m.features = str_to_title(cc.genes$g2m.genes),
  s.features = str_to_title(cc.genes$s.genes),
  search=TRUE
)

#Getting the expression data from the seurat object
exp.matr <- as.matrix(s.data[["RNA_name"]]@data)
#Adding the two 'genes' in the end of the matrix
matr <- rbind(exp.matr, s.data@meta.data$S.Score)
rownames(matr)[nrow(matr)] <- "S"
matr <- rbind(matr, s.data@meta.data$G2M.Score)
rownames(matr)[nrow(matr)] <- "G2m"
# scale the data 
matr <- t(scale(t(matr)))
```

```{r Calculating NT type estimation per cell}
# Getting cell-type by cell matrix.
es.max <- sctype_score(scRNAseqData = matr, scaled = TRUE, gs = NT.type.markers$gs_positive, gs2 = NT.type.markers$gs_negative)

s.data <- AddMetaData(object=s.data,metadata = apply(es.max,2,function(x){names(which.max(x))}), col.name = "NT.type")
```

# Recalculate LSI and UMAP

```{r Recalculate LSI after scRNA mapping score subsetting}
DefaultAssay(s.data) <- "peaks"

s.data <- RunTFIDF(s.data, method=3)
s.data <- FindTopFeatures(s.data, min.cutoff = 'q5')
s.data <- RunSVD(
  object = s.data,
  assay = 'peaks',
  reduction.key = 'LSI_',
  reduction.name = 'lsi',
  n=60
)
```

```{r Recalculate proportion of stdebv covered by LSI components in comparison to 2}
std.proportion <- sapply(2:length(s.data@reductions$lsi@stdev),function(x){s.data@reductions$lsi@stdev[x]/s.data@reductions$lsi@stdev[2]})

# Finding number of components of which std proportion to 2nd component is closest to std.aim
max.lsi.dim.re <- which.min(abs(std.proportion-std.aim))
```

```{r Recalculate UMAP and community detection}
s.data <- RunUMAP(object = s.data, reduction = 'lsi', dims = 2:max.lsi.dim.re)
s.data <- FindNeighbors(object = s.data, reduction = 'lsi', dims = 2:max.lsi.dim.re)

suppressWarnings(s.data <- FindClusters(object = s.data, verbose = FALSE, algorithm = 4, resolution = 4, random.seed=2020))

DimPlot(object = s.data, label = TRUE, pt.size=1.2, label.size = 8) + NoLegend()
```


```{r Recalculate UMAP and clustering in iterative loop}
NT.purity.values <- c()
for (r in seq(0.7,3,by=0.1)){
  print(paste("Processing resolution ",r,sep=""))
  # Recalculate clustering with resolution r
  s.data <- FindClusters(object = s.data, verbose = FALSE, algorithm = 4, resolution = r, random.seed=2020)

  #Calculate purity of NT types in clusters
  NT.conf.table <- table(s.data$seurat_clusters,s.data$NT.type)
  NT.purity.values<-c(NT.purity.values,sum(apply(NT.conf.table,1,function(x){max(x)}))/sum(NT.conf.table))
  print(NT.purity.values)
}

#DimPlot(object = s.data, label = TRUE, pt.size=1.2, label.size = 10) + NoLegend()

```

---

Final UMAP after filtering cells based on prediction score


```{r}
create_dt(as.data.frame.matrix(NT.conf.table))
```


```{r chooseR to validate r for FindClusters}
source("../../Github/chooseR/R/pipeline.R")

npcs <- 58
#resolutions <- c(0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 2.2, 2.4,2.6,2.8,3,3.2,3.4,3.6,3.8,4,5,6,7,8,9,10)
resolutions.cont <- c(1.2, 1.8, 2.4,3,3.2,3.6,3.8,4.5,5,6,7,8,9,10)
assay <- "peaks"
reduction <- "lsi"
results_path <- "results/"
```


```{r For loop version}
# Run pipeline

for (res in resolutions) {
  message(paste0("Clustering ", res, "..."))
  message("\tFinding ground truth...")

  # "Truths" will be stored at glue::glue("{reduction}.{assay}_res.{res}")
  s.data <- find_clusters(
                       s.data,
                       reduction = reduction,
                       assay = assay,
                       resolution = res,
                       npcs=npcs
  )
  clusters <- s.data[[glue::glue("{reduction}.{assay}_res.{res}")]]

  # Now perform iterative, sub-sampled clusters
  results <- multiple_cluster(
                              s.data,
                              n = 100,
                              size = 0.8,
                              npcs = npcs,
                              res = res,
                              reduction = reduction,
                              assay = assay
  )

  # Now calculate the co-clustering frequencies
  message(paste0("Tallying ", res, "..."))
  # This is the more time efficient vectorisation
  # However, it exhausts vector memory for (nearly) all datasets
  # matches <- purrr::map(columns, find_matches, df = results)
  # matches <- purrr::reduce(matches, `+`)
  columns <- colnames(dplyr::select(results, -cell))
  mtchs <- matrix(0, nrow = dim(results)[1], ncol = dim(results)[1])
  i <- 1 # Counter
  for (col in columns) {
    message(paste0("\tRound ", i, "..."))
    mtchs <- Reduce("+", list(
      mtchs,
      find_matches(col, df = results)
    ))
    i <- i + 1
  }

  message(paste0("Scoring ", res, "..."))
  mtchs <- dplyr::mutate_all(
    dplyr::as_tibble(mtchs),
    function(x) dplyr::if_else(Re(x) > 0, percent_match(x), 0)
  )

  # Now calculate silhouette scores
  message(paste0("Silhouette ", res, "..."))
  sil <- cluster::silhouette(
    x = as.numeric(as.character(unlist(clusters))),
    dmatrix = (1 - as.matrix(mtchs))
  )
  saveRDS(sil, paste0(results_path, "silhouette_", res, ".rds"))

  # Finally, calculate grouped metrics
  message(paste0("Grouping ", res, "..."))
  grp <- group_scores(mtchs, unlist(clusters))
  saveRDS(grp, paste0(results_path, "frequency_grouped_", res, ".rds"))
  sil <- group_sil(sil, res)
  saveRDS(sil, paste0(results_path, "silhouette_grouped_", res, ".rds"))

  }


```

```{r mclapply parallel version}
# Run pipeline
require(parallel)
require(Seurat)
require(Signac)
loop.run.res <- mclapply(resolutions.cont, function(res) {
  message(paste0("Clustering ", res, "..."))
  message("\tFinding ground truth...")

  # "Truths" will be stored at glue::glue("{reduction}.{assay}_res.{res}")
  s.data <- find_clusters(
                       s.data,
                       reduction = reduction,
                       assay = assay,
                       resolution = res,
                       npcs=npcs
  )
  clusters <- s.data[[glue::glue("{reduction}.{assay}_res.{res}")]]

  # Now perform iterative, sub-sampled clusters
  results <- multiple_cluster(
                              s.data,
                              n = 100,
                              size = 0.8,
                              npcs = npcs,
                              res = res,
                              reduction = reduction,
                              assay = assay
  )

  # Now calculate the co-clustering frequencies
  message(paste0("Tallying ", res, "..."))
  # This is the more time efficient vectorisation
  # However, it exhausts vector memory for (nearly) all datasets
  # matches <- purrr::map(columns, find_matches, df = results)
  # matches <- purrr::reduce(matches, `+`)
  columns <- colnames(dplyr::select(results, -cell))
  mtchs <- matrix(0, nrow = dim(results)[1], ncol = dim(results)[1])
  i <- 1 # Counter
  for (col in columns) {
    message(paste0("\tRound ", i, "..."))
    mtchs <- Reduce("+", list(
      mtchs,
      find_matches(col, df = results)
    ))
    i <- i + 1
  }

  message(paste0("Scoring ", res, "..."))
  mtchs <- dplyr::mutate_all(
    dplyr::as_tibble(mtchs),
    function(x) dplyr::if_else(Re(x) > 0, percent_match(x), 0)
  )

  # Now calculate silhouette scores
  message(paste0("Silhouette ", res, "..."))
  sil <- cluster::silhouette(
    x = as.numeric(as.character(unlist(clusters))),
    dmatrix = (1 - as.matrix(mtchs))
  )
  saveRDS(sil, paste0(results_path, "silhouette_", res, ".rds"))

  # Finally, calculate grouped metrics
  message(paste0("Grouping ", res, "..."))
  grp <- group_scores(mtchs, unlist(clusters))
  saveRDS(grp, paste0(results_path, "frequency_grouped_", res, ".rds"))
  sil <- group_sil(sil, res)
  saveRDS(sil, paste0(results_path, "silhouette_grouped_", res, ".rds"))

  return(list(res=res, clusters=clusters, mtchs=mtchs, results=results))
  }, mc.cores=2)

qsave(loop.run.res, "loop.run.res.cont.qs",nthreads = 10)

```

```{r}


# Save original data, with ground truth labels
#saveRDS(obj, paste0(results_path, "clustered_data.rds"))

# Create silhouette plot
# Read in scores and calculate CIs
scores <- purrr::map(
  paste0(results_path, "silhouette_grouped_", resolutions, ".rds"),
  readRDS
)
scores <- dplyr::bind_rows(scores) %>%
  dplyr::group_by(res) %>%
  dplyr::mutate("n_clusters" = dplyr::n()) %>%
  dplyr::ungroup()
meds <- scores %>%
  dplyr::group_by(res) %>%
  dplyr::summarise(
    "boot" = list(boot_median(avg_sil)),
    "n_clusters" = mean(n_clusters)
  ) %>%
  tidyr::unnest_wider(boot)

writexl::write_xlsx(meds, paste0(results_path, "median_ci.xlsx"))

# Find thresholds
threshold <- max(meds$low_med)
choice <- as.character(
  meds %>%
  dplyr::filter(med >= threshold) %>%
  dplyr::arrange(n_clusters) %>%
  tail(n = 1) %>%
  dplyr::pull(res)
)

# And plot!
ggplot(meds, aes(factor(res), med)) +
  geom_crossbar(
    aes(ymin = low_med, ymax = high_med),
    fill = "grey",
    size = 0.25
  ) +
  geom_hline(aes(yintercept = threshold), colour = "blue") +
  geom_vline(aes(xintercept = choice), colour = "red") +
  geom_jitter(
    data = scores,
    aes(factor(res), avg_sil),
    size = 0.35,
    width = 0.15
  ) +
  scale_x_discrete("Resolution") +
  scale_y_continuous(
    "Silhouette Score",
    expand = c(0, 0),
    limits = c(-1, 1),
    breaks = seq(-1, 1, 0.25),
    oob = scales::squish
  ) +
  cowplot::theme_minimal_hgrid() +
  theme(
    axis.title = element_text(size = 8),
    axis.text = element_text(size = 7),
    axis.line.x = element_line(colour = "black"),
    axis.line.y = element_line(colour = "black"),
    axis.ticks = element_line(colour = "black"),
  )

ggsave(
  filename = paste0(results_path, "silhouette_distribution_plot.png"),
  dpi = 300,
  height = 3.5,
  width = 3.5,
  units = "in"
)

# Finally, a dot plot of silhouette scores to help identify less robust clusters
# The initial pipe is to order the clusters by silhouette score
scores %>%
  dplyr::filter(res == choice) %>%
  dplyr::arrange(dplyr::desc(avg_sil)) %>%
  dplyr::mutate_at("cluster", ordered, levels = .$cluster) %>%
  ggplot(aes(factor(cluster), avg_sil)) +
    geom_point() +
    scale_x_discrete("Cluster") +
    scale_y_continuous(
      "Silhouette Score",
      expand = c(0, 0),
      limits = c(-1, 1),
      breaks = seq(-1, 1, 0.25),
      oob = scales::squish
    ) +
    cowplot::theme_minimal_grid() +
    theme(
      axis.title = element_text(size = 8),
      axis.text = element_text(size = 7),
      axis.line.x = element_line(colour = "black"),
      axis.line.y = element_line(colour = "black"),
      axis.ticks = element_line(colour = "black"),
    )

ggsave(
  filename = paste0(results_path, "silhouette_point_plot_", choice, ".png"),
  dpi = 300,
  height = 3.5,
  width = 3.5,
  units = "in"
)
```



### Plot marker gene FeaturePlots with imputed scRNA data

```{r Plot marker gene FeaturePlots with imputed scRNA data, fig.height=20, fig.width=20, cache=TRUE, message=FALSE}
neuronal.markers<- read_tsv("../../CellAnnotation/CellTypeMarkers_E14.tsv", col_names = c("annotation","geneName"))
feature.metadata <- s.data[['Activity']][[]] %>% rownames_to_column(var="gene_id") %>% as_tibble()
neuronal.markers.tmp <- filter(feature.metadata, feature_symbol %in% neuronal.markers$geneName)

neuronal.markers.ids <- pull(neuronal.markers.tmp,"gene_id")
neuronal.markers.names <- pull(neuronal.markers.tmp,"feature_symbol")
DefaultAssay(s.data) <- 'RNA'

f.plot.tmp <- FeaturePlot(
  object = s.data,
  features = neuronal.markers.ids,
  pt.size = 0.1,
  max.cutoff = 'q95',
  combine = F
)

f.plots.2 <- lapply(1:length(f.plot.tmp),function(i){
  f.plot.tmp[[i]] + labs(title=neuronal.markers.names[i])
})

patchwork::wrap_plots(f.plots.2)
```

---

Plot marker gene FeaturePlots with imputed scRNA data

### Plot gini index based validation of clustering effectiveness

```{r Plot gini index based validation of clustering effectiveness, message=FALSE}
# Define a set of HK genes
hk.genes <- c("RRN18S","Actb","Gapdh","Pgk1","Ppia","Rpl13a","Rplp0","Arbp","B2M","Ywhaz","Sdha","Tfrc","Gusb","Hmbs","Hprt1","Tbp")

hk.genes.id <- convert_feature_identity(s.data, "RNA",features = hk.genes)
neuronal.markers.id <- convert_feature_identity(s.data, "RNA",features = neuronal.markers$geneName)

# Neuronal marker mean per cluster in Cusanovich data
gene.i <- match(neuronal.markers.id,s.data[['RNA']]@data@Dimnames[[1]])
gene.i<-gene.i[!is.na(gene.i)]
barcode.clusters <- s.data@meta.data$seurat_clusters
marker.matrix <- s.data[['RNA']]@data[gene.i,]
marker.tb <- as_tibble(t(as.data.frame(marker.matrix)))
marker.tb<-tibble(marker.tb,cluster=barcode.clusters)
marker.mean <- list()
marker.mean$mean.by.cluster <- marker.tb %>% group_by(cluster) %>% summarize_all(mean)

# HK gene mean per cluster in Cusanovich data
gene.i <- match(hk.genes.id,s.data[['RNA']]@data@Dimnames[[1]])
gene.i<-gene.i[!is.na(gene.i)]
barcode.clusters <- s.data@meta.data$seurat_clusters
marker.matrix <- s.data[['RNA']]@data[gene.i,]
marker.tb <- as_tibble(t(as.data.frame(marker.matrix)))
marker.tb<-tibble(marker.tb,cluster=barcode.clusters)
hk.mean <- list()
hk.mean$mean.by.cluster <- marker.tb %>% group_by(cluster) %>% summarize_all(mean)

# Gini indeces for Cusanovich data
cus.hk.gini <- apply(hk.mean$mean.by.cluster[,-1],2,gini)
cus.neur.gini <- apply(marker.mean$mean.by.cluster[,-1],2,gini)

gini.tb<-tibble(gini.index=c(cus.hk.gini,cus.neur.gini),type=c(rep("hk",length(cus.hk.gini)),rep("neur",length(cus.neur.gini)))) %>%  dplyr::filter(!is.na(gini.index))

ggplot(gini.tb, aes(x=gini.index,y=type,fill="blue"))+geom_boxplot(fill="lightblue")+ theme(legend.position="none") + theme_classic()
```

---

Gini index based validation of clustering.


### Calculating known expression programs

```{r Calculating known expression programs as modules}
# Reading in CellTypes table
# cell_types <- read_xlsx("../../Cell types_210712.xlsx",sheet="neuron types")
# e.prog.info <- filter(cell_types, `brain region`=="DI") %>% select(`label (RNA)`, `scRNA cluster markers (top 20)`) %>% distinct()
# 
# DefaultAssay(s.data) <- "RNA"
# 
# genes.of.programs <- sapply(pull(e.prog.info,  `scRNA cluster markers (top 20)`),function(x){
#   tmp.genes <- unlist(str_split(x,pattern = ", "))
#   convert_feature_identity(object=s.data, assay="RNA", features=tmp.genes, feature.format = "symbol")
#   })
# 
# s.data <- AddModuleScore(s.data, features = genes.of.programs, assay="RNA", name=paste("ExpProg.",pull(e.prog.info, `label (RNA)`),sep=""))
```

### Adding Motif information into the object

```{r Adding Motif information to the object}
Hocomocov11 <- read_jaspar("../../mm10/HOCOMOCOv11_core_MOUSE_mono_jaspar_format.txt")
names(Hocomocov11) <- lapply(Hocomocov11,function(x){x@name})
Hocomocov11 <- convert_motifs(Hocomocov11, "TFBSTools-PWMatrix")
PWMs <- do.call(PWMatrixList,Hocomocov11)
DefaultAssay(s.data) <- "peaks"

# add motif information
s.data <- Signac::AddMotifs(
  object = s.data,
  genome = BSgenome.Mmusculus.UCSC.mm10,
  pfm = PWMs
)
```

```{r Run ChromVar and save data object}
s.data <- RunChromVAR(
  object = s.data,
  genome = BSgenome.Mmusculus.UCSC.mm10
)
```

```{r Find Closest features for ATAC features}
DefaultAssay(s.data) <- "peaks"
closest.features <- ClosestFeature(s.data, regions=rownames(s.data))
saveRDS(closest.features,file=paste("../analysis/",sample.name,"_nmm_closest_features.",run.date,".Rds",sep=""))
```

### Identification of markers for clusters defined based on various modalities

```{r Identification of UMAP cluster markers, message=FALSE}
# We need to run detection separately for both modality and then combine via AUC score

DefaultAssay(s.data) <- "RNA"
markers_rna <- as_tibble(presto:::wilcoxauc.Seurat(X = s.data, group_by = "seurat_clusters", assay = 'data', seurat_assay = 'RNA'))

DefaultAssay(s.data) <- "peaks"
markers_atac <- as_tibble(presto:::wilcoxauc.Seurat(X = s.data, group_by = "seurat_clusters", assay = 'data', seurat_assay = 'peaks'))

markers.atac.annotated <- left_join(markers_atac, as_tibble(closest.features), by=c("feature"="query_region"))
```

```{r Find markers for clusters based on chromvar, message=FALSE}
DefaultAssay(s.data) <- "chromvar"
markers_chromvar <- as_tibble(FindAllMarkers(
  object = s.data,
  only.pos = TRUE,
  test.use = 'LR',
  latent.vars = 'nCount_peaks'
)) %>% filter(p_val_adj <= 0.01 & avg_log2FC >= 0.75)
```

```{r Finding overrepresented motifs among markers_atac features, message=FALSE}
DefaultAssay(s.data) <- "peaks"
motif.markers <- markers.atac.annotated %>% filter(logFC > 0.5 & padj <= 0.01) %>% group_by(group) %>% select(feature, group) %>% group_modify(~FindMotifs(object=s.data, features=.x$feature)) %>% filter(pvalue <= 0.01 & fold.enrichment >= 1.5)
```

```{r Calculating top markers for each modality}
# Now we need to combine markers_rna, markers_atac, markers_chromvar, motif.markers in meaningful output to help with cluster annotation

# Writing marker info out to be used separately, writing out top 100
top.markers_rna <- as_tibble(markers_rna) %>% dplyr::filter(padj <= 0.01) %>% group_by(group) %>% top_n(n = 25, wt = logFC)
top.markers_rna$feature_symbol <- convert_feature_identity(s.data, "RNA", feature.format="ens", features = pull(top.markers_rna,feature))

top.markers_atac <- as_tibble(markers.atac.annotated) %>% dplyr::filter(padj <= 0.01) %>% group_by(group) %>% top_n(n = 25, wt = logFC)

top.markers_chromvar <- as_tibble(markers_chromvar) %>% dplyr::filter(p_val_adj <= 0.01) %>% group_by(cluster) %>% top_n(n = 25, wt = avg_log2FC)

top.markers_motifs <- as_tibble(motif.markers) %>% dplyr::filter(pvalue <= 0.01) %>% group_by(group) %>% top_n(n = 25, wt = fold.enrichment)

save(list=c("top.markers_rna","top.markers_atac","top.markers_chromvar","top.markers_motifs","closest.features"), file=paste("../analysis/",sample.name,"_RNApos_ATAC_cluster_markers.",run.date,".RData", sep=""))
```


```{r Fething cell type labels for idents}
# cell.type.data <- readxl::read_xlsx(path = "/Users/kilpinen/OneDrive - University of Helsinki/Analysis results/LATEST/Cell lineages/Cell types master table 081121 joint feature space.xlsx", sheet="cell types labels") %>% filter(`Brain region` == "DI")
# all.idents <- tibble(idents=Idents(s.data)) %>%  mutate_all(.funs=as.numeric)
# cell.types <- left_join(all.idents, cell.type.data, by=c("idents"="Cluster")) %>% dplyr::select(`Cell type`)
# 
# s.data <- AddMetaData(s.data, metadata = pull(cell.types,`Cell type`), col.name = "CellTypes")
```


### Saving data for downstream analyses

```{r Saving data for downstream analyses}
saveRDS(s.data,paste("../scATAC_data/",sample.name,"_DownstreamReady.",run.date,".RNA.pos.idents.Rds",sep=""))
```

```{r Saving data for downstream analyses slim}
s.data.slim <- s.data
s.data.slim[['peaks_count']] <- NULL
s.data.slim[['Activity']] <- NULL
saveRDS(s.data.slim,paste("../scATAC_data/",sample.name,"_DownstreamReady.",run.date,".RNA.pos.idents_slim.Rds",sep=""))
```

```{r}
# conda env r411_291021
sessionInfo()
```


#Tables {data-icon="fa-table"}
=====================================

### Cross-tabulation (confusion matrix) in to what extent each scRNA based cell type is included in each scATAC cluster.
#
```{r Cross tabulation between scATAC and scRNA based clustering}
scATAC.clusters <- Idents(s.data)
scRNA.clusters <- s.data@meta.data$predicted.id

conf.mat <- table(as.factor(scRNA.clusters),scATAC.clusters)
create_dt(as.data.frame.matrix(conf.mat))
```
#
# ---
#
# Cross-tabulation (confusion matrix) in to what extent each scRNA based cell type is included in each scATAC cluster.
#
#
# ### Table format of the top1 gene expression by scATAC cluster dotplot
#
# ```{r, message=FALSE}
# create_dt(as.data.frame.matrix(d.plot$data))
# ```
#
# ---
#
# ### Top 20 marker genes per scATAC cluster
#
```{r Top 20 marker genes per scATAC cluster}
#top.20.markers.by.cluster <- atac.expression.markers %>% group_by(cluster) %>% top_n(n = 20, wt = avg_log2FC)
create_dt(as.data.frame.matrix(data.frame(top.markers_rna)))
```
#
# ---
#
# Marker table filtered for convenience having top 20 genes pre cluster
#
# ### scRNA cluster marker genes (top20 per cluster)
#
# ```{r scRNA cluster marker genes, message=FALSE}
# clean_top20_markers <- read_tsv(scRNA_clean_markers_file)
# create_dt(clean_top20_markers)

```

---
